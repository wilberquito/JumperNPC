\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[catalan]{babel}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfiles}
\usepackage{comment}
\usepackage{titlesec}
% caption and subcaption work together
\usepackage{subcaption} % loads the caption package
\setcounter{secnumdepth}{4}

% Code
\usepackage{listings}
\usepackage{minted}
\usepackage{xcolor}

% Ploting 
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{booktabs}
\usepackage{array}
\usepackage{colortbl}

% Ploting settings
\pgfplotstableset{ 
  % global config, for example in the preamble
  every head row/.style={before row=\toprule,after row=\midrule},
  every last row/.style={after row=\bottomrule},
  fixed,precision=2,
}

% Code settings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codedarkgray}{rgb}{0.75,0.75,0.75}
\definecolor{codepurple}{rgb}{1,0,0}
\definecolor{codedarkpurple}{rgb}{0.65,0,0.8}
\definecolor{codedarkorange}{rgb}{0.85,0.4,0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.95}

\lstdefinestyle{mystyle}{
    frame=tb,
    language=[Sharp]C,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray},
    keywordstyle=\color{codedarkorange},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codegreen},
    basicstyle=\fontsize{9}{10}\selectfont\ttfamily,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    literate={\ \ }{{\ }}1
}
\lstset{style=mystyle}

\setlength{\parskip}{0.5em}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

%%------- Definició de l'estructura del titol -------%%
\title{ \textbf{ Aprenentatge per reforç amb Unity} 
\\ {\Large TAIA Project}}
\author{\small Wilber Eduardo Bermeo Quito }
%\date{\small Entrega: 5 de decembre 2021}


% Canvi de nom a la taula de continguts
\renewcommand*\contentsname{Taula de continguts}
\renewcommand\vdots{\vbox{\baselineskip3pt\lineskiplimit0pt\kern1pt\hbox{.}\hbox{.}\hbox{.}\kern-1pt}}

%%------- Inici del documents -------%%
\begin{document}

\maketitle

 % Titol
    \maketitle
    \thispagestyle{empty}
    \newpage
    
    % Taula de continguts
    \tableofcontents
    \newpage
    
    %% ---- start documentation ---- %%
    
    \section{Problema a resoldre en una frase}
    
    Fer que un NPC segueixi una política en concret fent us de Deep Reinforcement Learning en Unity.

    \section{Introducció i Motivació}
    
    Durant l’estiu del 2021 m’he estat barallant amb Unity (motor de videojocs) perquè vull fer un joc com a treball final de grau.
    
    El joc, resumidament, serà un Metroidvania 2D, en el qual hi haurà una tira de diferents NPCs. Molts d’aquests NPCs seran personatges actius i enemics.

    La motivació es clara, aprendre més d'Unity i expandir la riquesa del TFG amb les eines pròpies d’Unity que dona per implementar intel·ligència artificial als videojocs.  
    
    \section{Dades/Coneixements dels que es disposa}
    
    Degüt a la naturalesa del problema seleccionat, no es necessiten conjunts de dades per poder treballar.

    Coneixo l’entorn de treball Unity, no a nivell professional, però si tinc un base. 
    
    Lo que hem permetrà encarar el problema es l'eina \textit{ML-Agents Toolkit}. Dona totes les eines necessàries per utilitzar Unity com a motor de simulació per que els Agents de les escenes aprenguin polítiques segons una modelització del problema a resoldre.
    
    \newpage
    
    \section{Proposta}
    
    \subsection{Agent}
    Classe que conte mètodes que poden ser sobre escrits. La seva API contempla mètodes per poder generar observacions del medi, per prendre accions i per assignar recompenses.
    
    La classe Agent conte altres mètodes que poden ser sobre escrits. Cada Agent esta relacionat amb un Behavior Parameter class i a un Decision Requester.
    
    \subsubsection{Como modelitzar un Agent?}
    Es necessiten definir tres tipus d’entitats per cada moment en el joc.
    
    \begin{itemize}
        \item Observacions
        \item Accions
        \item Recompenses
    \end{itemize}
    
    \paragraph{Observacions}
    
    Les observacions poden ser numèriques i/o visuals. Les observacions numèriques són del punt de vista del observador (NPC), les visuals en canvi, són generades per col·lisións de rajos que es poden ajuntada al agent i representen el que l’agent esta veient en aquell mateix moment.
    
    \paragraph{Accions}
    
    Lo que realment l’agent pot fer dintre de l’escena. Les accions poden ser representades per valors continus o discrets.
    
    \paragraph{Recompenses}
    Es un escalar que representa que tan bé ho esta fent l’agent. Les recompenses (negatives o positives) no tenen perquè ser donades a cada moment, es poden donar en certs moments provocats per esdeveniments. Es important saber quan i quant es recompensa o es castiga a l’agent pel seus actes. Depenenet de la política de recompenses farà que l’aprenentatge de una certa tasca sigui mes ràpida o més lenta.
    
    \subsection{Polítiques a aprendre}
    Els següents dos punts son els problemes a modelitzar perquè l'Agent aprengui la politica adecuada per compliar-los.
    
    \subsubsection{Autonomia en el moviment}
    
    L´NPC ha ser capaç de moure d’esquerra a dreta de manera cíclica. El radi de desplaçament respecte el punt origen ha de ser configurable.
    
    Arribar al màxim rang de desplaçament tant d’esquerra a la dreta ha de donar feetback positiu, però no ha de passar que es quedi en un extrem sense moure, només s’ha de poder atorgar feetback positiu si s´ha vingut de l’altre extrem, un cas excepcional seria la primera vegada que apareix en l’entorn del joc ja que surt del centre del desplaçament.
    
    En cas de col·lisions amb objectes que no siguin targets d'atac dintre del seu rang ha de poder ser capaç de girar i fer el recorregut contrari.
    
    \subsubsection{Saber atacar}
    
    Un cop l'Agent hagi apres a moure's, la meva idea es que l’NPC detecti entitats a atacar.
    
    Aquestes entitats únicament seran detectables si estan dintre d’un rang esfèric que tindrà com radi el rang de desplaçament del NPC. L’objectiu en aquest cas es que, deixi de fer el moviment cíclic d’esquerra detrac i que salti sobre l’objectiu fent col·lisió, les col·lisions encertades donaran feetback positiu.
    
    \newpage
    
    \subsection{Arquitectura, Algorismes, Propostes existents}
    
    La arquitectura del problema que abordo be donat per les eines que utilitzo per resoldre-ho, en aquest cas \textit{ML-Agents Toolkit}. A part de les eines de \textit{ML-Agents Toolkit} faré servir \textit{Tensorboard} per veure les estadistiques d'aprenentage.
    
    A continuació explicaré com es l'arquitectura de ML-Agents Toolkit. ML-Agents Toolkit te els següents quatre blocks com a estructura principal.
    
    \begin{itemize}
        \item Espai d'aprenentatge (Escena d'Unity)
        \item Comuniador (Broquer que connecta Unity amb l'API de Python)
        \item Python API
        \item Python Trainers
    \end{itemize}

    \includegraphics[width=\textwidth]{images/learning_environment_basic.png}
    
    \newpage
    
    \subsubsection{Espai d'aprenentatge}
    Aquí esta l’escena d’Unity i els personatges. Els personatges que per requeriment del problema s’han definits com Agents, podrán observar, prendre desicions i tenir recompenses.
    
    \subsubsection{Agent}
    Objecte d'Unity que hereta de la clase \textit{Agent} amb metodes que poden ser sobrescrits. 
    
    La seva API contempla mètodes per poder generar observacions del medi, per prendre accions i per assignar recompenses.
    
    Cada Agent esta relacionat amb un Behavior Parameter class i a un Decision Requester.
    
    Els metodes més comuns a sobrescriure de la clase \textit{Agent} són els següents:
    \begin{itemize}
        \item Initialize
        \item OnEpisodiBegin
        \item OnActionReceived
        \item CollectObservations
        \item Heuristic
    \end{itemize}
    
    \newpage
    
    \paragraph{Initialize}
    Aquest mètode es crida un cop únicament en el cicle de vida del aprenentatge.Esta pensat per fer caching d’objectes de l'escena o inicialitzar valors.
    
    \paragraph{OnEpisodiBegin}
    Aquest mètode es crida cada cop que un episodi d’aprenentatge ha acabat, forçadament o perquè el màxim nombre de passos del episodi s’han completat.
    
    \paragraph{OnActionReceived}
    Aquest mètode porta un paràmetre d’entrada, anomenat ActionBuffer, l’ActionBuffer, es un objecte que te dos arrays, un per poder representar valors continus i altres per representar valors discrets.
    
    Aquest mètode interactua tan amb el mètode \textit{heuristic} i amb el motor d'aprenentatge automàtic.
    
    \paragraph{CollectObservations}
    Aquest mètode s’executa cada x temps, aquest temps d’execució es configura amb l’eina de Unity.
    
    El mètode te com a paràmetre una  estructura de dades semblants com a paràmetre d'entrada que el mètode OnActionReceived. En aquest cas no esta pensat per consumir les dades sinó per modificar aquesta referencia i per donar informació al motor de Reinforcement Learning quan estem en mode de \textit{Learning}.

    
    \paragraph{Heuristic}
    Aquest mètode permet interactuar amb el jugador. Fixat que te la mateixa estructura de dades com a paràmetre que el mètode ActionBuffers.
    
    Els valors que s'enplenin en els buffers d'aquesta estructura de dades aniran a parar al mètode \textit{OnActionReceived}.
    
    \subsubsection{Behaviour}
    Script que esta pensat per parametritzar i relacionar l’Agent amb la configuració externa a Unity.
    
    Un Behavior pot tenir els següents comportaments:
    \begin{itemize}
        \item Learning
        \item Heuristic
        \item Inference
    \end{itemize}
    
    \paragraph{Learning}
    Quan el \textit{Behavior} esta en mode \textit{Learning} vol dir que utilitza el model generat per la sobrescritura dels metodes de la clase \textit{Agent} per aprendre. En aquest mode Unity es connecta amb el procés de \textit{mlagents-learn} per generar la xarxa neuronal que representa l'aprenentatge del model.
    
    \paragraph{Heuristic}
    En aquest mode no hi ha procés d'aprenentatge, s'utilitza la funcio \textit{heuristic} de la clase \textit{Agent} per interacturar amb l'Agent.
    
    \newpage
    
    \paragraph{Inference}
    Quan el \textit{Behavior} esta en mode \textit{Inference} implica que hi ha una xarxa neuronal amb una politica apresa, i l'objecte que conte aquesta xarxa neuronal es comportarà segons lo que s'hagi apres.
    
    \subsubsection{Requester}
    Es un Script que serveix per forçar que l’Agent prengui decisions, sense aquest Script com a Component en el nostre Agent, l’Agent mai prendrà decisions. Aquí es pot configurar el període de decisions.
    
    
    En la següent imatge hi ha les tres diferents arquitectures que podem tenir segons quin comportament tingui el component \textit{Behavior} del objecte Agent.
    
    \includegraphics[width=\textwidth]{images/arquitecture_mlagent.png}
    
    \newpage
    
    \subsubsection{Tensorboard}
    Quan l’entrenament s’està executant, ML-Agents Toolkit guarda estadístiques dintre del directori results en un directori amb nom únic, donat a l’hora d’executar el procés mlagents-learn.
    
    Per observar el procés d’entrenament quan esta corrent el procés o quan no, ens apropem a la carpeta de resultats i executem: tensorboard –logdir */results\$\{training id\}.
    
    Això obrirà un servidor per servir el contingut estàtic en un dels ports de la teva màquina.
    
    Explicarè alguna de les gràfiques que \textit{Tensorboard} genera ja que molt segurament les veurem més endavant en cada un de les versions del Agent.
    
    \paragraph{Cumulative Reward}
    Gràfica que representa la mitja de lo que guanya l'Agent per episodi. Un bon procès d'aprenentatge genera una gràfica ascendent que en algun moment s'estanca ja que la seva mitja per episodi es mante.
    
    \paragraph{Episodi Length}
    Representa la durada mitja per episodi. Aquesta gràfica depen de l'implementació del model ja que segons la caràcteristica del problema es pot forçar el reinici de episodi, o potser el problema es màximitzar o minimitzar el temps de vida del Agent.
    
    \paragraph{Policy Loss}
    Representa quan la política del Agent canvia, un model correcte tendirà a fer una gràfica descendent.
    
    \paragraph{Entropy}
    Representa que tan aleatoris són els moviments del Agent, un model correcte expresa una gràfica d'Entropia descendent ja que significa que realment esta aprenent una política i no esta fent tants d'intents per probar coses noves.
    
    \newpage
    
    \textbf{Arquitectura final}
    
    \includegraphics[width=\textwidth]{images/arquitectura-tools.drawio.png} 
    
    \newpage
    
    \section{Experiments}
    
    Aquest projecte conté un seguit d'experiments baçats en fases.
    Les fases benen donades pel l'apartat \textit{4.2 Polítiques a apendre}.
    
    \subsection{ Representació del objecte NPC dintre d'Unity }
    L'objecte NPC esta format tres objectes fills.
    
    \begin{itemize}
        \item Limit esquerra
        \item Limit dret
        \item Agent
        \begin{itemize}
            \item Raig esquerra
            \item Raig dret
        \end{itemize}
    \end{itemize}

    Els objectes de limits, son objectes buits i invisibles que tenen un comportament de trigger de events en cas de col·lisións. Aquests objectes son escencials per poder delimitar el rang de moviment del Agent.
    
    L'objecte Agent conte el Script de modelatge que hereta de la clase \textit{Agent} a més dels Scripts \textit{Behavior Parameters} i \textit{DesitionRequester}. 
    
    Finalment tenim els rajos, que interaccionen amb elements de l'escana. El funcionament d'aquests rajos s'explica en l'apartat \textit{4.1.1.1 Observacions}. \\
    
    \includegraphics[width=\textwidth]{images/NPC_scene.png} 
    
    \newpage

    \subsection{ Autonomia en el moviment }
    
    \subsubsection{ Versió (I) }
    
    Els metodes que s'han sobrescrit de la clase \textit{Agent} són els següents:
    
    \begin{itemize}
        \item Initialize
        \item OnEpisodiBegin
        \item OnActionReceived
        \item CollectObservations
        \item Heuristic
    \end{itemize}
    
    Altres mètodes de clase no propis de la clase \textit{Agent} i que són interesants per poder explicar la modelització del problema:
    
    \begin{itemize}
        \item OnTriggerEnter2D
        \item OnTriggerExit2D
    \end{itemize}
    
    \paragraph{Initialize}
    
    Rutina que s'executa només un cop quan es dona play al motor d'Unity.
    
    Agafo el component del objecte que em permet treballar amb les fisiques de Unity. El component es el \textit{RigidBody} que ha de haver estat afegit al objecte com una component desde l'interficie d'Unity. El component \textit{RigidBody} ha de tenir el comportament de \textit{Dynamic} perque sigui afectat per la gravetat de configurada en l'escena.
    
    Per altra banda configuro el nombre de passos per episisodi a zero, en cas que l'estat de la variable \textit{trainingMode} sigui fals. Aquesta configuració em permet fer proves en mode \textit{Heuristic} o en mode \textit{Inferencia} sense pasar pel cicle de vida \textit{OnEpisidoBegin}. 
    
    La variable \textit{MaxStep} es una variable propia de la clase \textit{Agent} i representa el nombre de passos màxims que es fan quan l'Agent esta aprenent una política. Quan s'ha fet aquests nombres de passos, inmediatament pel cicle de vida de l'Agent, força l'execució de la rutina \textit{OnEpisodiBegin}. \\
    
    \begin{lstlisting}
    public override void Initialize()
    {
        _rigidbody2D = GetComponent<Rigidbody2D>();
        // infinite steps for session
        if (!trainningMode) MaxStep = 0;
    }
    \end{lstlisting}
    
    
    \newpage
    
    \paragraph{OnEpisodeBegin}
    
    El següent métode s'executa al inici de cada episodi. Es important de restablir els estats inicials d'algunes propietats de l'Agent, degüt a que l'Agent mante la inercia del episodi anterior.
    
    En la rutina es treu l'inercia que tenia l'Agent en el episodi anterior, s'el reposiciona al centre i s'escull una meta aleatoriament. A aquestes altures les metes a escollir són els limits.
    
    \begin{lstlisting}
    public override void OnEpisodeBegin()
    {
        // reseting movement inercy
        _rigidbody2D.velocity = Vector2.zero;
        //reseting positions
        transform.position = transform.parent.position;
        // changin randomnes
        Random.InitState(System.DateTime.Now.Millisecond);
        // finding the moving target
        FindMovingTarget();
    }
    \end{lstlisting}
    
    \paragraph{OnActionReceived}
    
    La lògica dintre d'aquests métode determina el comportament de l'Agent.
    Les accions que s'esperen es una continua, un valor entre menys u i més u. Segons aquesta acció la velocitat del Agent serà donada.
    
    \begin{lstlisting}
    // called when action is received from either {player, neural network}
    // each buffer position refers to an action, I decide what it means for each positions
    // inside that structure has continuous and discrete actions
    // index 0: -1 means move to the left, +1 means move to the right
    // the cool thing about the neural network, is that it figures it all automatic
    public override void OnActionReceived(ActionBuffers actions)
    {
        Vector2 movement = new Vector2(actions.ContinuousActions[0] * movementForce, 0);
        _rigidbody2D.velocity = movement;
    }
    \end{lstlisting}
    
    \paragraph{CollectObservations}
    
    En aquest métode li he de donar al procés d'aprenentatge aquells paràmetres que jo crec que són importants per tenir en compte a l'hora d'aprendre la política.
    
    Aqui vaig trobar interesant d'informar al procès d'aprenentatge, quina es la horientació del Agent respecte el punt objectiu. La distancia del Agent respecte el punt objectiu, la posició del punt objectiu i finalment la velicitat de moviment del Agent. 
    
    \begin{lstlisting}
    // Should include all variables relevant for following 
    // to take the agent the optimally informed desition.
    // No extraneous information here please
    public override void CollectObservations(VectorSensor sensor)
    {
        if (!currentTarget) return;

        Vector2 currentPos = new Vector2(transform.position.x, 0);
        Vector2 targetPos = new Vector2(currentTarget.position.x, 0);

        Vector2 toTarget = targetPos - currentPos;
        // 2 observations (horientation)
        sensor.AddObservation(toTarget.normalized);
        // 1 observation (distance)
        sensor.AddObservation(Vector2.Distance(targetPos, currentPos));
        // 2 observations for current target position
        sensor.AddObservation(targetPos);
        // 2 observations for movement velocity
        sensor.AddObservation(_rigidbody2D.velocity);
        // Note: curiosamente si normalizo la velocidad, le cuesta mucho aprenderx
    }
    \end{lstlisting}
   
   \newpage
   
   \paragraph{Heuristic}
   En aquesta versió únicament l'Agent ha de poder interpretar valors entrats per teclat que representin moviments laterals.
   
   Es modifica la referéncia dels \textit{ActionBuffers} de tal manera que en el buffer de variables continues es pasa el valor entrat per teclat. \\
   
   \begin{lstlisting}
   // this method allows me to interact with the game
    // when the ml agents is not set to trainning
    public override void Heuristic(in ActionBuffers actionsOut)
    {
        ActionSegment<float> continuosActions = actionsOut.ContinuousActions;
        var force = Input.GetAxis("Horizontal");
        continuosActions[0] = force;
    }
   \end{lstlisting}
   
   \newpage
    
    \subsection{Metodologia i Resultats}
    
    \subsubsection{Autonomia en el moviment, versió (I)}
    L'objectiu de fer aprendre la política de moures ciclacament ha sigut tot un exit. 
    
    Perque el temps d'aprenentatge es fes més curt, el que vaig fer va ser duplicar l'Agent quatre vegades. \\
    
    \includegraphics[width=\textwidth]{images/v1/autonomia_v1.jpeg}
    
    Fixem-nos en els següents logs que \textit{mlagents-learn} va soltant en el procés d'aprenentatge. \\
    
    \includegraphics[width=\textwidth]{images/v1/log_v1.jpeg}
    
    Cada una de les entrades del log representen episodis. Les primeres entrades donen una mitja \textit{Mean Reward} que les últimes i la desviació estandard es bastant més ample que les últimes.
    
    Finalment arriba el moment on l'aprenentatge de l'Agent no canvia. Això vol dir que ha après una política i inmediatament, \textit{mlagents-learn} quan veu que la mitja per cada episodi es la mitja tanca el procés d'aprenentatge.
    
    \newpage
    
    A continuació veurem algunes gràfiques que \textit{Tensorboard} ens ha generat respecte aquest procés d'aprenentatge.
    
    \begin{figure}[H]
        \begin{figure}[H]
        \centering
            \subcaptionbox{Comulative reward}{\includegraphics[width=0.30\textwidth]{images/v1/comulative_reward_autonomia_v1.png}}%
            \hfill % <-- Seperation
            \subcaptionbox{Episodi length}{\includegraphics[width=0.30\textwidth]{images/v1/episode_length_autonomia_v1.png}}%
            \hfill % <-- Seperation
            \subcaptionbox{Learning rate}{\includegraphics[width=0.30\textwidth]{images/v1/learning_rate_v1.png}}%
            \hfill % <-- Seperation
             \subcaptionbox{Policity loss}{\includegraphics[width=0.30\textwidth]{images/v1/policity_loss_autonomia_v1.png}}%
            \hfill % <-- Seperation
            \subcaptionbox{Value Loss}{\includegraphics[width=0.30\textwidth]{images/v1/value_loss_autonomia_v1.png}}%
            \hfill % <-- Seperation
            \subcaptionbox{Entropy}{\includegraphics[width=0.30\textwidth]{images/v1/entropy_autonomia_v1.png}}%
            
        \end{figure}
    \end{figure}

          
    \section{Conclusions i línies futures}
    
    \section{Referències}

\end{document}
